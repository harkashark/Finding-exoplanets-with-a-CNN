{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note to users:<h6> Before running, change file path names as appropriate. </h6>\n",
    "\n",
    "Throughout I reference code adapted from Peter Grenholm's model which can be found on Kaggle <a href=\"https://www.kaggle.com/toregil/mystery-planet-99-8-cnn\">here</a>, along with the training and test datasets. The rest, which is not so referenced, is my own. His implementation is less compute-intensive, but this model below produces better results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.ndimage.filters import uniform_filter1d\n",
    "\n",
    "import keras\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding, Flatten, BatchNormalization\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "\n",
    "import sklearn.metrics as skl\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pickle as pkl\n",
    "import math\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading the data.\n"
     ]
    }
   ],
   "source": [
    "exotrain = pd.read_csv('kepler-labelled-time-series-data\\exoTrain.csv')\n",
    "exotest = pd.read_csv('kepler-labelled-time-series-data\\exoTest.csv')\n",
    "print('Done loading the data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train, labels_test = exotrain.LABEL-1, exotest.LABEL-1 #relabelling: yes_planet=1 and no_planet=0 instead of 2, 1 respectively\n",
    "exotrain, exotest = exotrain.drop('LABEL',axis=1), exotest.drop('LABEL',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get general idea of the shape and features in the data, it is sensible to explore graphically a sample of stars with planets (hereinafter ‘positive examples’) and without planets (hereinafter ‘negative examples’)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_np = labels_train.loc[labels_train==1].shape[0]\n",
    "titles_p = []\n",
    "titles_np = []\n",
    "for starnum in range(8):\n",
    "    titles_p.append('Star with planet {}'.format(starnum))\n",
    "    titles_np.append('Star without planet {}'.format(starnum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22b73269b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting stars WITH planets\n",
    "image_p = plt.figure()\n",
    "for starnum in range(8):\n",
    "    plt.subplot(2,4,starnum+1)\n",
    "    plt.title(titles_p[starnum])\n",
    "    plt.axis('off')\n",
    "    exotrain.iloc[starnum,:].transpose().plot(legend = None)\n",
    "    plt.grid(True)\n",
    "plt.subplots_adjust(top=0.92, bottom=0, left=-2, right=0.95, hspace=0.25, wspace=0.35)\n",
    "image_p.savefig('stars_with_planets.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22b73269630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting stars WITHOUT planets\n",
    "image_np = plt.figure()\n",
    "for starnum in range(8):\n",
    "    plt.subplot(2,4,starnum+1)\n",
    "    plt.title(titles_np[starnum])\n",
    "    plt.axis('off')\n",
    "    exotrain.iloc[first_np+starnum,:].transpose().plot(legend = None, color='r')\n",
    "    plt.grid(True)\n",
    "plt.subplots_adjust(top=0.92, bottom=0, left=-2, right=0.95, hspace=0.25, wspace=0.35)\n",
    "image_np.savefig('stars_without_planets.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply normalisation to both the training and the test data, star by star. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_rows(input_df):\n",
    "    for index in range(input_df.shape[0]):\n",
    "        input_df.iloc[index, :] = (input_df.iloc[index, :] - input_df.iloc[index, :].mean()) / input_df.iloc[index, :].std()\n",
    "    return input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalisation complete.\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = normalize_rows(exotrain), normalize_rows(exotest), labels_train, labels_test\n",
    "print('normalisation complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I double the size of the positive examples by reversing them in time and adding the reversed examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives = x_train.loc[y_train==1]\n",
    "reversed_positives = positives[positives.columns[::-1]]\n",
    "reversed_positives.columns = positives.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we add them to the start and do the same for their labels and relabel\n",
    "x_train = pd.concat([reversed_positives, x_train])\n",
    "y_train = pd.concat([y_train.loc[y_train==1], y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We convert the dataframes to np arrays for ease of manipulation later\n",
    "x_train = pd.DataFrame.as_matrix(x_train)\n",
    "x_test = pd.DataFrame.as_matrix(x_test)\n",
    "y_train = pd.DataFrame.as_matrix(y_train)\n",
    "y_test = pd.DataFrame.as_matrix(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll get everything into the right shape before feeding it into the CNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we reshape the data for input to the CNN\n",
    "y_train = np.expand_dims(y_train, axis=1)\n",
    "y_test = np.expand_dims(y_test, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we add channels of varying smoothness. This is a very important step as it allows us the CNN to look at both the noisy data and smooth data, which has information over longer timescales than just the small kernels on the noisy data. \n",
    "\n",
    "Credit to Peter Grenholm on Kaggle for this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the smoothed timeseries as a second dimension.\n",
    "x_train = np.stack([x_train, uniform_filter1d(x_train, axis=1, size=200)], axis=2)\n",
    "x_test = np.stack([x_test, uniform_filter1d(x_test, axis=1, size=200)], axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> The Convolutional Neural Net (CNN) </h1>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need an approach to modelling that is (1) able to extract subtle planetary features (i.e. patterns that mark a star as having a planet); (2) able to distinguish these features from noise; (3) able to identify the regularity of these features; (4) scalable to a large number of examples; (5) insensitive to different periodicities of planetary features across examples; (6) insensitive to (or removes) macroscopic and microscopic noise, some of which itself may be regular. Additionally, there may be planetary features hidden in the data that are not obvious to humans, and it would be ideal if our model could employ these features too for classification. \n",
    "\n",
    "A CNN meets these specifications. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we build the CNN and run the experiments, we'll define a few functions and variables that we'll use in the experiments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we build a batch generator (adapted from Peter Grenholm on Kaggle), since without custom-defining batches, default-random batches will consist almost entirely of negative examples and the signal for positive examples will be drowned out. An alternative might be adding an exceptionally large penalty for false negatives (for example, by labelling positive examples with 100 instead of 1). But this method is sufficient to yield good results. For further data augmentation, this function also rotates the examples in time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(x_train, y_train, batch_size=32):\n",
    "    \"\"\"\n",
    "    Gives equal number of positive and negative samples, and rotates them randomly in time\n",
    "    \"\"\"\n",
    "    half_batch = batch_size // 2\n",
    "    x_batch = np.empty((batch_size, x_train.shape[1], x_train.shape[2]), dtype='float32')\n",
    "    y_batch = np.empty((batch_size, y_train.shape[1]), dtype='float32')\n",
    "    \n",
    "    yes_idx = np.where(y_train[:,0] == 1.)[0]\n",
    "    non_idx = np.where(y_train[:,0] == 0.)[0]\n",
    "    while True:\n",
    "        np.random.shuffle(yes_idx)\n",
    "        np.random.shuffle(non_idx)\n",
    "    \n",
    "        x_batch[:half_batch] = x_train[yes_idx[:half_batch]]\n",
    "        x_batch[half_batch:] = x_train[non_idx[half_batch:batch_size]]\n",
    "        y_batch[:half_batch] = y_train[yes_idx[:half_batch]]\n",
    "        y_batch[half_batch:] = y_train[non_idx[half_batch:batch_size]]\n",
    "    \n",
    "        for i in range(batch_size):\n",
    "            sz = np.random.randint(x_batch.shape[1])\n",
    "            x_batch[i] = np.roll(x_batch[i], sz, axis = 0)\n",
    "     \n",
    "        yield x_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the experiments for-loop, we will reset our model every time by rebuilding it, calling this function.  \n",
    "\n",
    "Credit to Peter Grenholm for many features of the architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=10, kernel_size=11, activation='relu', input_shape=x_train.shape[1:]))\n",
    "    model.add(MaxPooling1D(strides=4))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(filters=20, kernel_size=11, activation='relu'))\n",
    "    model.add(MaxPooling1D(strides=4))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(filters=40, kernel_size=11, activation='relu'))\n",
    "    model.add(MaxPooling1D(strides=4))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv1D(filters=80, kernel_size=11, activation='relu'))\n",
    "    model.add(MaxPooling1D(strides=4))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using the same learning rate for each epoch, we will half the learning rate every 10 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.05\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our positive example are very few, it might be the case that taking out certain important examples for use in the validation set might affect learning. To test for this, I use a different validation set every experiment by running this function in the experiment for-loop: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_set_generator(x_train, y_train, test_size=0.10):  \n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=test_size, shuffle=True, stratify=y_train)\n",
    "    return (x_train, x_val, y_train, y_val)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing training sets in variables of different names. This step is just\n",
    "# for convenience and can be skipped if memory is a constraint, but make sure\n",
    "# to change the names of variables in later steps as appropriate.\n",
    "\n",
    "x_train2, y_train2 = x_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll define the batch size, the number of experiments, and some callbacks that will define the early-stopping and learning rate decay processes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialise variables for the experiment\n",
    "b_size = 32\n",
    "\n",
    "#how many experiment we want to run\n",
    "experiments = range(0, 10)\n",
    "\n",
    "#Define our hyperparameters for early stopping and learning-rate decay\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001, patience=20, verbose=1, mode='min')   \n",
    "lr_schedule = keras.callbacks.LearningRateScheduler(step_decay, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Running the experiments</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time per experiment will vary.\n",
    "\n",
    "Filepath names should be changed as appropriate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment #0\n",
      "Epoch 1/300\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 8s 80ms/step - loss: 0.6227 - acc: 0.6695 - val_loss: 0.5120 - val_acc: 0.7096\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.5407 - acc: 0.7326 - val_loss: 0.1304 - val_acc: 0.9552\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.4666 - acc: 0.7850 - val_loss: 0.5735 - val_acc: 0.7076\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.3678 - acc: 0.8466 - val_loss: 0.1694 - val_acc: 0.9318\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.2390 - acc: 0.9037 - val_loss: 0.1672 - val_acc: 0.9454\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.1609 - acc: 0.9362 - val_loss: 0.3560 - val_acc: 0.8986\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.1120 - acc: 0.9640 - val_loss: 0.1800 - val_acc: 0.9532\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.1245 - acc: 0.9621 - val_loss: 0.1506 - val_acc: 0.9688\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.1015 - acc: 0.9710 - val_loss: 0.1331 - val_acc: 0.9630\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0610 - acc: 0.9807 - val_loss: 0.0977 - val_acc: 0.9805\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0492 - acc: 0.9855 - val_loss: 0.0987 - val_acc: 0.9766\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0554 - acc: 0.9836 - val_loss: 0.0984 - val_acc: 0.9786\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0561 - acc: 0.9801 - val_loss: 0.1280 - val_acc: 0.9669\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0523 - acc: 0.9830 - val_loss: 0.0790 - val_acc: 0.9766\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0380 - acc: 0.9890 - val_loss: 0.0645 - val_acc: 0.9883\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0533 - acc: 0.9839 - val_loss: 0.0865 - val_acc: 0.9747\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0511 - acc: 0.9848 - val_loss: 0.0509 - val_acc: 0.9864\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0327 - acc: 0.9899 - val_loss: 0.0719 - val_acc: 0.9844\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0395 - acc: 0.9886 - val_loss: 0.0871 - val_acc: 0.9844\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0236 - acc: 0.9946 - val_loss: 0.0841 - val_acc: 0.9825\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0386 - acc: 0.9912 - val_loss: 0.0614 - val_acc: 0.9883\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0250 - acc: 0.9924 - val_loss: 0.0782 - val_acc: 0.9883\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0254 - acc: 0.9924 - val_loss: 0.0835 - val_acc: 0.9883\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0321 - acc: 0.9915 - val_loss: 0.0659 - val_acc: 0.9883\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0257 - acc: 0.9940 - val_loss: 0.0714 - val_acc: 0.9883\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 3s 25ms/step - loss: 0.0249 - acc: 0.9924 - val_loss: 0.0667 - val_acc: 0.9883\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0236 - acc: 0.9927 - val_loss: 0.0778 - val_acc: 0.9883\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0219 - acc: 0.9940 - val_loss: 0.0831 - val_acc: 0.9883\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0222 - acc: 0.9937 - val_loss: 0.0659 - val_acc: 0.9903\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0216 - acc: 0.9931 - val_loss: 0.0577 - val_acc: 0.9903\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0189 - acc: 0.9940 - val_loss: 0.0597 - val_acc: 0.9903\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0270 - acc: 0.9924 - val_loss: 0.0520 - val_acc: 0.9922\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0222 - acc: 0.9940 - val_loss: 0.0501 - val_acc: 0.9903\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0271 - acc: 0.9927 - val_loss: 0.0411 - val_acc: 0.9942\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0258 - acc: 0.9931 - val_loss: 0.0481 - val_acc: 0.9903\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0153 - acc: 0.9959 - val_loss: 0.0539 - val_acc: 0.9883\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0167 - acc: 0.9953 - val_loss: 0.0466 - val_acc: 0.9903\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.0149 - acc: 0.9968 - val_loss: 0.0486 - val_acc: 0.9903\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0232 - acc: 0.9931 - val_loss: 0.0504 - val_acc: 0.9903\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0253 - acc: 0.9924 - val_loss: 0.0486 - val_acc: 0.9922\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 0.003125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0123 - acc: 0.9953 - val_loss: 0.0509 - val_acc: 0.9903\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0168 - acc: 0.9943 - val_loss: 0.0505 - val_acc: 0.9903\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0255 - acc: 0.9931 - val_loss: 0.0515 - val_acc: 0.9903\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0184 - acc: 0.9949 - val_loss: 0.0463 - val_acc: 0.9922\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0148 - acc: 0.9962 - val_loss: 0.0454 - val_acc: 0.9922\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0127 - acc: 0.9962 - val_loss: 0.0492 - val_acc: 0.9922\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0164 - acc: 0.9956 - val_loss: 0.0474 - val_acc: 0.9922\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0236 - acc: 0.9937 - val_loss: 0.0488 - val_acc: 0.9903\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0129 - acc: 0.9965 - val_loss: 0.0514 - val_acc: 0.9883\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0186 - acc: 0.9940 - val_loss: 0.0431 - val_acc: 0.9903\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0118 - acc: 0.9959 - val_loss: 0.0472 - val_acc: 0.9922\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0148 - acc: 0.9940 - val_loss: 0.0485 - val_acc: 0.9922\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0139 - acc: 0.9959 - val_loss: 0.0486 - val_acc: 0.9922\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0115 - acc: 0.9965 - val_loss: 0.0505 - val_acc: 0.9922\n",
      "Epoch 00054: early stopping\n",
      "Complete!\n",
      "Experiment #1\n",
      "Epoch 1/300\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 7s 72ms/step - loss: 0.6044 - acc: 0.6761 - val_loss: 0.4874 - val_acc: 0.7680\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.5277 - acc: 0.7437 - val_loss: 0.3615 - val_acc: 0.8655\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.4239 - acc: 0.8100 - val_loss: 0.2200 - val_acc: 0.9103\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.3057 - acc: 0.8775 - val_loss: 0.1605 - val_acc: 0.9454\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.2486 - acc: 0.9075 - val_loss: 0.2168 - val_acc: 0.9220\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.1777 - acc: 0.9400 - val_loss: 0.0999 - val_acc: 0.9786\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.1723 - acc: 0.9419 - val_loss: 0.2872 - val_acc: 0.9103\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.1264 - acc: 0.9564 - val_loss: 0.2077 - val_acc: 0.9415\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.1229 - acc: 0.9621 - val_loss: 0.2384 - val_acc: 0.9123\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0949 - acc: 0.9691 - val_loss: 0.1492 - val_acc: 0.9571\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0678 - acc: 0.9776 - val_loss: 0.1547 - val_acc: 0.9630\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0653 - acc: 0.9789 - val_loss: 0.1215 - val_acc: 0.9708\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0410 - acc: 0.9890 - val_loss: 0.0987 - val_acc: 0.9766\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0440 - acc: 0.9852 - val_loss: 0.1226 - val_acc: 0.9688\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0441 - acc: 0.9874 - val_loss: 0.1260 - val_acc: 0.9669\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0505 - acc: 0.9842 - val_loss: 0.1316 - val_acc: 0.9688\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0454 - acc: 0.9864 - val_loss: 0.1430 - val_acc: 0.9630\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0364 - acc: 0.9890 - val_loss: 0.1180 - val_acc: 0.9708\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0299 - acc: 0.9896 - val_loss: 0.1171 - val_acc: 0.9669\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0282 - acc: 0.9918 - val_loss: 0.0932 - val_acc: 0.9786\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0270 - acc: 0.9915 - val_loss: 0.1120 - val_acc: 0.9688\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0325 - acc: 0.9886 - val_loss: 0.1017 - val_acc: 0.9727\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.0243 - acc: 0.9924 - val_loss: 0.1022 - val_acc: 0.9708\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0220 - acc: 0.9924 - val_loss: 0.1008 - val_acc: 0.9766\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0196 - acc: 0.9956 - val_loss: 0.0912 - val_acc: 0.9786\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0310 - acc: 0.9908 - val_loss: 0.0867 - val_acc: 0.9786\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.0125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0247 - acc: 0.9915 - val_loss: 0.0808 - val_acc: 0.9766\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0261 - acc: 0.9899 - val_loss: 0.0656 - val_acc: 0.9864\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0197 - acc: 0.9953 - val_loss: 0.0940 - val_acc: 0.9766\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0291 - acc: 0.9912 - val_loss: 0.0678 - val_acc: 0.9844\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0176 - acc: 0.9949 - val_loss: 0.0810 - val_acc: 0.9825\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0141 - acc: 0.9953 - val_loss: 0.0722 - val_acc: 0.9844\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0228 - acc: 0.9940 - val_loss: 0.0758 - val_acc: 0.9825\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0203 - acc: 0.9924 - val_loss: 0.0788 - val_acc: 0.9786\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0266 - acc: 0.9896 - val_loss: 0.0633 - val_acc: 0.9864\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0155 - acc: 0.9965 - val_loss: 0.0816 - val_acc: 0.9844\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0269 - acc: 0.9893 - val_loss: 0.0648 - val_acc: 0.9864\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0196 - acc: 0.9937 - val_loss: 0.0858 - val_acc: 0.9805\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0214 - acc: 0.9927 - val_loss: 0.0654 - val_acc: 0.9883\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0202 - acc: 0.9946 - val_loss: 0.0634 - val_acc: 0.9883\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0126 - acc: 0.9972 - val_loss: 0.0678 - val_acc: 0.9864\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0142 - acc: 0.9953 - val_loss: 0.0748 - val_acc: 0.9864\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.0187 - acc: 0.9949 - val_loss: 0.0675 - val_acc: 0.9864\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0166 - acc: 0.9934 - val_loss: 0.0661 - val_acc: 0.9864\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.0167 - acc: 0.9943 - val_loss: 0.0674 - val_acc: 0.9864\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0131 - acc: 0.9953 - val_loss: 0.0680 - val_acc: 0.9844\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0170 - acc: 0.9956 - val_loss: 0.0717 - val_acc: 0.9844\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0168 - acc: 0.9946 - val_loss: 0.0628 - val_acc: 0.9903\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0123 - acc: 0.9946 - val_loss: 0.0599 - val_acc: 0.9903\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0182 - acc: 0.9943 - val_loss: 0.0648 - val_acc: 0.9883\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0193 - acc: 0.9934 - val_loss: 0.0597 - val_acc: 0.9903\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.0135 - acc: 0.9949 - val_loss: 0.0599 - val_acc: 0.9922\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0131 - acc: 0.9956 - val_loss: 0.0622 - val_acc: 0.9864\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0170 - acc: 0.9940 - val_loss: 0.0592 - val_acc: 0.9903\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0127 - acc: 0.9962 - val_loss: 0.0641 - val_acc: 0.9903\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0175 - acc: 0.9940 - val_loss: 0.0598 - val_acc: 0.9903\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0181 - acc: 0.9949 - val_loss: 0.0586 - val_acc: 0.9922\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0159 - acc: 0.9934 - val_loss: 0.0609 - val_acc: 0.9903\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.0186 - acc: 0.9949 - val_loss: 0.0565 - val_acc: 0.9922\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0126 - acc: 0.9956 - val_loss: 0.0576 - val_acc: 0.9922\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0173 - acc: 0.9953 - val_loss: 0.0603 - val_acc: 0.9903\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0113 - acc: 0.9978 - val_loss: 0.0570 - val_acc: 0.9922\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0140 - acc: 0.9962 - val_loss: 0.0586 - val_acc: 0.9922\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0134 - acc: 0.9965 - val_loss: 0.0563 - val_acc: 0.9903\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0108 - acc: 0.9972 - val_loss: 0.0591 - val_acc: 0.9922\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0236 - acc: 0.9924 - val_loss: 0.0591 - val_acc: 0.9903\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to 0.00078125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0130 - acc: 0.9962 - val_loss: 0.0572 - val_acc: 0.9922\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0198 - acc: 0.9943 - val_loss: 0.0560 - val_acc: 0.9922\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0194 - acc: 0.9940 - val_loss: 0.0602 - val_acc: 0.9922\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to 0.000390625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0121 - acc: 0.9972 - val_loss: 0.0604 - val_acc: 0.9922\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to 0.000390625.\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.0131 - acc: 0.9965 - val_loss: 0.0590 - val_acc: 0.9922\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to 0.000390625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0140 - acc: 0.9959 - val_loss: 0.0622 - val_acc: 0.9864\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to 0.000390625.\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.0142 - acc: 0.9962 - val_loss: 0.0619 - val_acc: 0.9903\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to 0.000390625.\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.0135 - acc: 0.9968 - val_loss: 0.0614 - val_acc: 0.9903\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to 0.000390625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0154 - acc: 0.9953 - val_loss: 0.0602 - val_acc: 0.9922\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to 0.000390625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0133 - acc: 0.9953 - val_loss: 0.0583 - val_acc: 0.9922\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to 0.000390625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0147 - acc: 0.9953 - val_loss: 0.0585 - val_acc: 0.9922\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to 0.000390625.\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.0147 - acc: 0.9956 - val_loss: 0.0597 - val_acc: 0.9922\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to 0.000390625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0139 - acc: 0.9965 - val_loss: 0.0577 - val_acc: 0.9922\n",
      "Epoch 00079: early stopping\n",
      "Complete!\n",
      "Experiment #2\n",
      "Epoch 1/300\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 7s 75ms/step - loss: 0.6230 - acc: 0.6556 - val_loss: 0.5759 - val_acc: 0.6862\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.5414 - acc: 0.7292 - val_loss: 0.5293 - val_acc: 0.7271\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.4121 - acc: 0.8150 - val_loss: 0.3969 - val_acc: 0.8207\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.2848 - acc: 0.8876 - val_loss: 0.3425 - val_acc: 0.8947\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.2396 - acc: 0.9132 - val_loss: 0.2080 - val_acc: 0.9123\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.1829 - acc: 0.9331 - val_loss: 0.1840 - val_acc: 0.9201\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.1503 - acc: 0.9460 - val_loss: 0.3546 - val_acc: 0.8674\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.1171 - acc: 0.9596 - val_loss: 0.1748 - val_acc: 0.9376\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0826 - acc: 0.9729 - val_loss: 0.1372 - val_acc: 0.9688\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0763 - acc: 0.9782 - val_loss: 0.1157 - val_acc: 0.9708\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0652 - acc: 0.9801 - val_loss: 0.1105 - val_acc: 0.9727\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0513 - acc: 0.9817 - val_loss: 0.1296 - val_acc: 0.9669\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0520 - acc: 0.9864 - val_loss: 0.0936 - val_acc: 0.9766\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0453 - acc: 0.9839 - val_loss: 0.0535 - val_acc: 0.9903\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0513 - acc: 0.9861 - val_loss: 0.0937 - val_acc: 0.9825\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0502 - acc: 0.9848 - val_loss: 0.1272 - val_acc: 0.9727\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0495 - acc: 0.9880 - val_loss: 0.0548 - val_acc: 0.9864\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0298 - acc: 0.9896 - val_loss: 0.0826 - val_acc: 0.9825\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0383 - acc: 0.9880 - val_loss: 0.0676 - val_acc: 0.9786\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0254 - acc: 0.9921 - val_loss: 0.0703 - val_acc: 0.9786\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0284 - acc: 0.9902 - val_loss: 0.0850 - val_acc: 0.9727\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0290 - acc: 0.9924 - val_loss: 0.0630 - val_acc: 0.9805\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0300 - acc: 0.9896 - val_loss: 0.0554 - val_acc: 0.9825\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0338 - acc: 0.9896 - val_loss: 0.0738 - val_acc: 0.9805\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0242 - acc: 0.9924 - val_loss: 0.0571 - val_acc: 0.9844\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0199 - acc: 0.9943 - val_loss: 0.0583 - val_acc: 0.9903\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0268 - acc: 0.9921 - val_loss: 0.0532 - val_acc: 0.9864\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.0125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0335 - acc: 0.9918 - val_loss: 0.0593 - val_acc: 0.9864\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0201 - acc: 0.9937 - val_loss: 0.0554 - val_acc: 0.9883\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0122 - acc: 0.9959 - val_loss: 0.0562 - val_acc: 0.9883\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0214 - acc: 0.9953 - val_loss: 0.0463 - val_acc: 0.9864\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0187 - acc: 0.9956 - val_loss: 0.0553 - val_acc: 0.9864\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0243 - acc: 0.9931 - val_loss: 0.0505 - val_acc: 0.9864\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0221 - acc: 0.9949 - val_loss: 0.0557 - val_acc: 0.9844\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0206 - acc: 0.9937 - val_loss: 0.0578 - val_acc: 0.9844\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0299 - acc: 0.9908 - val_loss: 0.0478 - val_acc: 0.9883\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0217 - acc: 0.9934 - val_loss: 0.0516 - val_acc: 0.9864\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0221 - acc: 0.9943 - val_loss: 0.0525 - val_acc: 0.9844\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0178 - acc: 0.9943 - val_loss: 0.0448 - val_acc: 0.9883\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0159 - acc: 0.9946 - val_loss: 0.0441 - val_acc: 0.9883\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0184 - acc: 0.9940 - val_loss: 0.0495 - val_acc: 0.9864\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0139 - acc: 0.9949 - val_loss: 0.0511 - val_acc: 0.9864\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0144 - acc: 0.9959 - val_loss: 0.0474 - val_acc: 0.9864\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0154 - acc: 0.9949 - val_loss: 0.0460 - val_acc: 0.9864\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0158 - acc: 0.9949 - val_loss: 0.0489 - val_acc: 0.9864\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0160 - acc: 0.9959 - val_loss: 0.0466 - val_acc: 0.9864\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0104 - acc: 0.9972 - val_loss: 0.0436 - val_acc: 0.9903\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0158 - acc: 0.9959 - val_loss: 0.0440 - val_acc: 0.9883\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0121 - acc: 0.9956 - val_loss: 0.0407 - val_acc: 0.9903\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0167 - acc: 0.9953 - val_loss: 0.0420 - val_acc: 0.9903\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0148 - acc: 0.9953 - val_loss: 0.0461 - val_acc: 0.9883\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.0216 - acc: 0.9946 - val_loss: 0.0438 - val_acc: 0.9903\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0125 - acc: 0.9975 - val_loss: 0.0448 - val_acc: 0.9864\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.0151 - acc: 0.9959 - val_loss: 0.0418 - val_acc: 0.9883\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0141 - acc: 0.9949 - val_loss: 0.0415 - val_acc: 0.9903\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0101 - acc: 0.9968 - val_loss: 0.0438 - val_acc: 0.9883\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 22ms/step - loss: 0.0129 - acc: 0.9959 - val_loss: 0.0425 - val_acc: 0.9883\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0111 - acc: 0.9959 - val_loss: 0.0441 - val_acc: 0.9883\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0141 - acc: 0.9956 - val_loss: 0.0437 - val_acc: 0.9864\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0121 - acc: 0.9965 - val_loss: 0.0432 - val_acc: 0.9864\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0113 - acc: 0.9965 - val_loss: 0.0451 - val_acc: 0.9864\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0107 - acc: 0.9968 - val_loss: 0.0455 - val_acc: 0.9864\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0150 - acc: 0.9959 - val_loss: 0.0446 - val_acc: 0.9864\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0130 - acc: 0.9968 - val_loss: 0.0456 - val_acc: 0.9864\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0185 - acc: 0.9946 - val_loss: 0.0444 - val_acc: 0.9864\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0141 - acc: 0.9962 - val_loss: 0.0447 - val_acc: 0.9864\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.0146 - acc: 0.9965 - val_loss: 0.0464 - val_acc: 0.9864\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to 0.00078125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0104 - acc: 0.9972 - val_loss: 0.0456 - val_acc: 0.9864\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0158 - acc: 0.9943 - val_loss: 0.0464 - val_acc: 0.9864\n",
      "Epoch 00069: early stopping\n",
      "Complete!\n",
      "Experiment #3\n",
      "Epoch 1/300\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 7s 75ms/step - loss: 0.6241 - acc: 0.6496 - val_loss: 0.4970 - val_acc: 0.7524\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.5163 - acc: 0.7513 - val_loss: 0.3000 - val_acc: 0.8421\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.3770 - acc: 0.8333 - val_loss: 0.2997 - val_acc: 0.8577\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.2444 - acc: 0.9085 - val_loss: 0.2762 - val_acc: 0.8830\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.2058 - acc: 0.9233 - val_loss: 0.1161 - val_acc: 0.9532\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.1712 - acc: 0.9422 - val_loss: 0.1036 - val_acc: 0.9669\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.1315 - acc: 0.9621 - val_loss: 0.1047 - val_acc: 0.9630\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.1143 - acc: 0.9602 - val_loss: 0.1138 - val_acc: 0.9552\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0819 - acc: 0.9710 - val_loss: 0.0895 - val_acc: 0.9688\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0439 - acc: 0.9845 - val_loss: 0.1069 - val_acc: 0.9727\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0516 - acc: 0.9839 - val_loss: 0.0597 - val_acc: 0.9786\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0374 - acc: 0.9890 - val_loss: 0.0856 - val_acc: 0.9727\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0403 - acc: 0.9893 - val_loss: 0.0638 - val_acc: 0.9844\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0451 - acc: 0.9836 - val_loss: 0.0862 - val_acc: 0.9708\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0372 - acc: 0.9893 - val_loss: 0.1163 - val_acc: 0.9688\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0278 - acc: 0.9908 - val_loss: 0.0602 - val_acc: 0.9825\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.0333 - acc: 0.9871 - val_loss: 0.0678 - val_acc: 0.9825\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.0453 - acc: 0.9852 - val_loss: 0.0806 - val_acc: 0.9688\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0310 - acc: 0.9896 - val_loss: 0.0553 - val_acc: 0.9805\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0246 - acc: 0.9934 - val_loss: 0.0640 - val_acc: 0.9786\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0291 - acc: 0.9899 - val_loss: 0.0658 - val_acc: 0.9805\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0251 - acc: 0.9927 - val_loss: 0.0731 - val_acc: 0.9805\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0255 - acc: 0.9912 - val_loss: 0.0890 - val_acc: 0.9766\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0177 - acc: 0.9946 - val_loss: 0.0715 - val_acc: 0.9805\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0301 - acc: 0.9915 - val_loss: 0.0768 - val_acc: 0.9766\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0175 - acc: 0.9946 - val_loss: 0.0554 - val_acc: 0.9825\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0264 - acc: 0.9921 - val_loss: 0.0776 - val_acc: 0.9786\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0242 - acc: 0.9908 - val_loss: 0.0614 - val_acc: 0.9864\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0269 - acc: 0.9943 - val_loss: 0.0566 - val_acc: 0.9844\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0223 - acc: 0.9940 - val_loss: 0.0554 - val_acc: 0.9844\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0165 - acc: 0.9946 - val_loss: 0.0617 - val_acc: 0.9805\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0266 - acc: 0.9927 - val_loss: 0.0686 - val_acc: 0.9805\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0194 - acc: 0.9937 - val_loss: 0.0762 - val_acc: 0.9805\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0179 - acc: 0.9940 - val_loss: 0.0560 - val_acc: 0.9844\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0182 - acc: 0.9953 - val_loss: 0.0571 - val_acc: 0.9844\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0171 - acc: 0.9943 - val_loss: 0.0608 - val_acc: 0.9844\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0260 - acc: 0.9927 - val_loss: 0.0612 - val_acc: 0.9844\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0178 - acc: 0.9937 - val_loss: 0.0755 - val_acc: 0.9825\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.00625.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0199 - acc: 0.9927 - val_loss: 0.0596 - val_acc: 0.9844\n",
      "Epoch 00039: early stopping\n",
      "Complete!\n",
      "Experiment #4\n",
      "Epoch 1/300\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 9s 91ms/step - loss: 0.5988 - acc: 0.6765 - val_loss: 0.4456 - val_acc: 0.7700\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.4683 - acc: 0.7771 - val_loss: 0.5285 - val_acc: 0.7173\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.3418 - acc: 0.8573 - val_loss: 0.1432 - val_acc: 0.9591\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.2604 - acc: 0.9047 - val_loss: 0.3552 - val_acc: 0.8752\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.1985 - acc: 0.9343 - val_loss: 0.0635 - val_acc: 0.9864\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.1518 - acc: 0.9498 - val_loss: 0.1315 - val_acc: 0.9591\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.1109 - acc: 0.9688 - val_loss: 0.0682 - val_acc: 0.9786\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.1220 - acc: 0.9640 - val_loss: 0.1503 - val_acc: 0.9357\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.1093 - acc: 0.9659 - val_loss: 0.0856 - val_acc: 0.9747\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0685 - acc: 0.9785 - val_loss: 0.0973 - val_acc: 0.9747\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0633 - acc: 0.9823 - val_loss: 0.1148 - val_acc: 0.9591\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0543 - acc: 0.9823 - val_loss: 0.0707 - val_acc: 0.9747\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0470 - acc: 0.9893 - val_loss: 0.0638 - val_acc: 0.9766\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0636 - acc: 0.9817 - val_loss: 0.0528 - val_acc: 0.9825\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0506 - acc: 0.9845 - val_loss: 0.0627 - val_acc: 0.9805\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0460 - acc: 0.9845 - val_loss: 0.0560 - val_acc: 0.9786\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0512 - acc: 0.9855 - val_loss: 0.0604 - val_acc: 0.9844\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0401 - acc: 0.9877 - val_loss: 0.0500 - val_acc: 0.9805\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0400 - acc: 0.9902 - val_loss: 0.0639 - val_acc: 0.9766\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0295 - acc: 0.9912 - val_loss: 0.0404 - val_acc: 0.9883\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0299 - acc: 0.9924 - val_loss: 0.0453 - val_acc: 0.9864\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0357 - acc: 0.9883 - val_loss: 0.0446 - val_acc: 0.9864\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0270 - acc: 0.9924 - val_loss: 0.0522 - val_acc: 0.9825\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.0196 - acc: 0.9940 - val_loss: 0.0381 - val_acc: 0.9903\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0283 - acc: 0.9908 - val_loss: 0.0436 - val_acc: 0.9844\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0217 - acc: 0.9949 - val_loss: 0.0466 - val_acc: 0.9864\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0182 - acc: 0.9946 - val_loss: 0.0446 - val_acc: 0.9883\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0374 - acc: 0.9883 - val_loss: 0.0415 - val_acc: 0.9883\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0306 - acc: 0.9883 - val_loss: 0.0475 - val_acc: 0.9864\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0254 - acc: 0.9927 - val_loss: 0.0363 - val_acc: 0.9883\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0189 - acc: 0.9940 - val_loss: 0.0372 - val_acc: 0.9903\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0181 - acc: 0.9943 - val_loss: 0.0393 - val_acc: 0.9864\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0170 - acc: 0.9943 - val_loss: 0.0358 - val_acc: 0.9883\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0220 - acc: 0.9940 - val_loss: 0.0283 - val_acc: 0.9922\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0195 - acc: 0.9953 - val_loss: 0.0329 - val_acc: 0.9942\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0220 - acc: 0.9937 - val_loss: 0.0368 - val_acc: 0.9883\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0155 - acc: 0.9956 - val_loss: 0.0350 - val_acc: 0.9922\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0201 - acc: 0.9946 - val_loss: 0.0385 - val_acc: 0.9864\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0172 - acc: 0.9956 - val_loss: 0.0372 - val_acc: 0.9903\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 0.003125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0115 - acc: 0.9949 - val_loss: 0.0351 - val_acc: 0.9903\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0171 - acc: 0.9943 - val_loss: 0.0346 - val_acc: 0.9922\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0151 - acc: 0.9949 - val_loss: 0.0337 - val_acc: 0.9903\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0230 - acc: 0.9924 - val_loss: 0.0399 - val_acc: 0.9864\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0118 - acc: 0.9972 - val_loss: 0.0378 - val_acc: 0.9903\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0147 - acc: 0.9953 - val_loss: 0.0401 - val_acc: 0.9903\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0161 - acc: 0.9956 - val_loss: 0.0393 - val_acc: 0.9922\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0188 - acc: 0.9934 - val_loss: 0.0430 - val_acc: 0.9844\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0115 - acc: 0.9968 - val_loss: 0.0402 - val_acc: 0.9903\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0101 - acc: 0.9981 - val_loss: 0.0420 - val_acc: 0.9903\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0121 - acc: 0.9959 - val_loss: 0.0387 - val_acc: 0.9903\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0104 - acc: 0.9975 - val_loss: 0.0381 - val_acc: 0.9903\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0174 - acc: 0.9943 - val_loss: 0.0357 - val_acc: 0.9922\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0077 - acc: 0.9987 - val_loss: 0.0364 - val_acc: 0.9922\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0153 - acc: 0.9934 - val_loss: 0.0363 - val_acc: 0.9903\n",
      "Epoch 00054: early stopping\n",
      "Complete!\n",
      "Experiment #5\n",
      "Epoch 1/300\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 8s 83ms/step - loss: 0.6208 - acc: 0.6487 - val_loss: 0.2433 - val_acc: 0.9103\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.4741 - acc: 0.7847 - val_loss: 0.5487 - val_acc: 0.7251\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.3849 - acc: 0.8280 - val_loss: 0.3365 - val_acc: 0.8148\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.3542 - acc: 0.8542 - val_loss: 0.0814 - val_acc: 0.9864\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.3078 - acc: 0.8791 - val_loss: 0.1218 - val_acc: 0.9454\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.2437 - acc: 0.9078 - val_loss: 0.7885 - val_acc: 0.6316\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.2100 - acc: 0.9242 - val_loss: 0.3879 - val_acc: 0.8304\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.1877 - acc: 0.9337 - val_loss: 0.1068 - val_acc: 0.9669\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.1503 - acc: 0.9511 - val_loss: 0.1353 - val_acc: 0.9415\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0924 - acc: 0.9681 - val_loss: 0.1312 - val_acc: 0.9552\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0725 - acc: 0.9751 - val_loss: 0.0959 - val_acc: 0.9747\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0872 - acc: 0.9722 - val_loss: 0.0932 - val_acc: 0.9669\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0759 - acc: 0.9776 - val_loss: 0.0923 - val_acc: 0.9688\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0655 - acc: 0.9789 - val_loss: 0.1514 - val_acc: 0.9532\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0651 - acc: 0.9830 - val_loss: 0.1045 - val_acc: 0.9610\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0547 - acc: 0.9795 - val_loss: 0.1045 - val_acc: 0.9669\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0515 - acc: 0.9833 - val_loss: 0.0964 - val_acc: 0.9669\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0573 - acc: 0.9804 - val_loss: 0.0945 - val_acc: 0.9727\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0602 - acc: 0.9811 - val_loss: 0.1120 - val_acc: 0.9532\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0433 - acc: 0.9855 - val_loss: 0.0710 - val_acc: 0.9747\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0428 - acc: 0.9871 - val_loss: 0.0818 - val_acc: 0.9708\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0260 - acc: 0.9946 - val_loss: 0.0826 - val_acc: 0.9727\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0527 - acc: 0.9864 - val_loss: 0.0728 - val_acc: 0.9708\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0381 - acc: 0.9896 - val_loss: 0.0558 - val_acc: 0.9747\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0299 - acc: 0.9915 - val_loss: 0.0560 - val_acc: 0.9747\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.0125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0323 - acc: 0.9902 - val_loss: 0.0578 - val_acc: 0.9766\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0385 - acc: 0.9896 - val_loss: 0.0548 - val_acc: 0.9766\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0299 - acc: 0.9899 - val_loss: 0.0726 - val_acc: 0.9708\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0318 - acc: 0.9927 - val_loss: 0.0624 - val_acc: 0.9786\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0309 - acc: 0.9902 - val_loss: 0.0538 - val_acc: 0.9786\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0325 - acc: 0.9918 - val_loss: 0.0603 - val_acc: 0.9747\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0245 - acc: 0.9915 - val_loss: 0.0562 - val_acc: 0.9766\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0244 - acc: 0.9921 - val_loss: 0.0592 - val_acc: 0.9747\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0266 - acc: 0.9927 - val_loss: 0.0500 - val_acc: 0.9766\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0212 - acc: 0.9931 - val_loss: 0.0501 - val_acc: 0.9766\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0198 - acc: 0.9943 - val_loss: 0.0530 - val_acc: 0.9786\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0287 - acc: 0.9924 - val_loss: 0.0420 - val_acc: 0.9825\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0226 - acc: 0.9924 - val_loss: 0.0528 - val_acc: 0.9844\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0295 - acc: 0.9921 - val_loss: 0.0486 - val_acc: 0.9786\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0252 - acc: 0.9931 - val_loss: 0.0486 - val_acc: 0.9825\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0213 - acc: 0.9927 - val_loss: 0.0451 - val_acc: 0.9786\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0211 - acc: 0.9931 - val_loss: 0.0482 - val_acc: 0.9786\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0211 - acc: 0.9959 - val_loss: 0.0513 - val_acc: 0.9766\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0269 - acc: 0.9943 - val_loss: 0.0548 - val_acc: 0.9766\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0215 - acc: 0.9934 - val_loss: 0.0494 - val_acc: 0.9766\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0177 - acc: 0.9949 - val_loss: 0.0505 - val_acc: 0.9766\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0240 - acc: 0.9924 - val_loss: 0.0492 - val_acc: 0.9786\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0234 - acc: 0.9931 - val_loss: 0.0501 - val_acc: 0.9786\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0191 - acc: 0.9943 - val_loss: 0.0510 - val_acc: 0.9786\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0148 - acc: 0.9953 - val_loss: 0.0446 - val_acc: 0.9805\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0184 - acc: 0.9937 - val_loss: 0.0502 - val_acc: 0.9786\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0142 - acc: 0.9965 - val_loss: 0.0442 - val_acc: 0.9786\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0114 - acc: 0.9965 - val_loss: 0.0467 - val_acc: 0.9786\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0167 - acc: 0.9946 - val_loss: 0.0387 - val_acc: 0.9786\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0253 - acc: 0.9949 - val_loss: 0.0378 - val_acc: 0.9844\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0211 - acc: 0.9924 - val_loss: 0.0406 - val_acc: 0.9786\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0176 - acc: 0.9949 - val_loss: 0.0427 - val_acc: 0.9786\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0189 - acc: 0.9949 - val_loss: 0.0433 - val_acc: 0.9786\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0223 - acc: 0.9949 - val_loss: 0.0408 - val_acc: 0.9786\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0116 - acc: 0.9962 - val_loss: 0.0440 - val_acc: 0.9805\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0152 - acc: 0.9962 - val_loss: 0.0455 - val_acc: 0.9786\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0146 - acc: 0.9965 - val_loss: 0.0438 - val_acc: 0.9786\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0152 - acc: 0.9946 - val_loss: 0.0475 - val_acc: 0.9786\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0263 - acc: 0.9902 - val_loss: 0.0419 - val_acc: 0.9786\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0150 - acc: 0.9949 - val_loss: 0.0460 - val_acc: 0.9786\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to 0.00078125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0252 - acc: 0.9965 - val_loss: 0.0482 - val_acc: 0.9786\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0162 - acc: 0.9943 - val_loss: 0.0441 - val_acc: 0.9805\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0130 - acc: 0.9968 - val_loss: 0.0446 - val_acc: 0.9805\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0222 - acc: 0.9934 - val_loss: 0.0386 - val_acc: 0.9825\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to 0.000390625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0177 - acc: 0.9949 - val_loss: 0.0403 - val_acc: 0.9805\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to 0.000390625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0204 - acc: 0.9934 - val_loss: 0.0408 - val_acc: 0.9825\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to 0.000390625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0159 - acc: 0.9953 - val_loss: 0.0436 - val_acc: 0.9786\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to 0.000390625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0149 - acc: 0.9959 - val_loss: 0.0438 - val_acc: 0.9805\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to 0.000390625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0252 - acc: 0.9927 - val_loss: 0.0412 - val_acc: 0.9825\n",
      "Epoch 00074: early stopping\n",
      "Complete!\n",
      "Experiment #6\n",
      "Epoch 1/300\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 8s 84ms/step - loss: 0.6174 - acc: 0.6638 - val_loss: 0.4232 - val_acc: 0.8168\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.4821 - acc: 0.7860 - val_loss: 0.1213 - val_acc: 0.9513\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.3153 - acc: 0.8763 - val_loss: 0.1472 - val_acc: 0.9337\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.2313 - acc: 0.9097 - val_loss: 0.2424 - val_acc: 0.8869\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.1697 - acc: 0.9366 - val_loss: 0.2500 - val_acc: 0.8986\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.1263 - acc: 0.9574 - val_loss: 0.1014 - val_acc: 0.9649\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.1074 - acc: 0.9621 - val_loss: 0.1497 - val_acc: 0.9435\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.1108 - acc: 0.9665 - val_loss: 0.1051 - val_acc: 0.9532\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0929 - acc: 0.9700 - val_loss: 0.0457 - val_acc: 0.9786\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0621 - acc: 0.9807 - val_loss: 0.0658 - val_acc: 0.9727\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0404 - acc: 0.9871 - val_loss: 0.0373 - val_acc: 0.9844\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0482 - acc: 0.9867 - val_loss: 0.0272 - val_acc: 0.9864\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0352 - acc: 0.9905 - val_loss: 0.0397 - val_acc: 0.9864\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0372 - acc: 0.9861 - val_loss: 0.0698 - val_acc: 0.9747\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0450 - acc: 0.9833 - val_loss: 0.0584 - val_acc: 0.9708\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0375 - acc: 0.9883 - val_loss: 0.0393 - val_acc: 0.9825\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0429 - acc: 0.9864 - val_loss: 0.0554 - val_acc: 0.9727\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0406 - acc: 0.9855 - val_loss: 0.0315 - val_acc: 0.9864\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0418 - acc: 0.9842 - val_loss: 0.0326 - val_acc: 0.9844\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0263 - acc: 0.9902 - val_loss: 0.0231 - val_acc: 0.9922\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0214 - acc: 0.9927 - val_loss: 0.0297 - val_acc: 0.9883\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0182 - acc: 0.9953 - val_loss: 0.0167 - val_acc: 0.9922\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0307 - acc: 0.9908 - val_loss: 0.0239 - val_acc: 0.9903\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0200 - acc: 0.9934 - val_loss: 0.0182 - val_acc: 0.9922\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0303 - acc: 0.9890 - val_loss: 0.0235 - val_acc: 0.9903\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0328 - acc: 0.9893 - val_loss: 0.0264 - val_acc: 0.9903\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0208 - acc: 0.9940 - val_loss: 0.0076 - val_acc: 0.9961\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0141 - acc: 0.9968 - val_loss: 0.0235 - val_acc: 0.9922\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0275 - acc: 0.9912 - val_loss: 0.0406 - val_acc: 0.9864\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0169 - acc: 0.9953 - val_loss: 0.0139 - val_acc: 0.9942\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0201 - acc: 0.9931 - val_loss: 0.0073 - val_acc: 0.9961\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.00625.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0189 - acc: 0.9949 - val_loss: 0.0126 - val_acc: 0.9942\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0159 - acc: 0.9956 - val_loss: 0.0097 - val_acc: 0.9961\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0120 - acc: 0.9962 - val_loss: 0.0137 - val_acc: 0.9942\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0226 - acc: 0.9943 - val_loss: 0.0143 - val_acc: 0.9942\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0119 - acc: 0.9959 - val_loss: 0.0092 - val_acc: 0.9961\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0171 - acc: 0.9959 - val_loss: 0.0141 - val_acc: 0.9942\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0181 - acc: 0.9953 - val_loss: 0.0083 - val_acc: 0.9961\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0104 - acc: 0.9968 - val_loss: 0.0169 - val_acc: 0.9942\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0161 - acc: 0.9956 - val_loss: 0.0097 - val_acc: 0.9961\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0163 - acc: 0.9956 - val_loss: 0.0091 - val_acc: 0.9981\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0186 - acc: 0.9927 - val_loss: 0.0068 - val_acc: 0.9961\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0173 - acc: 0.9959 - val_loss: 0.0060 - val_acc: 0.9961\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0173 - acc: 0.9949 - val_loss: 0.0082 - val_acc: 0.9961\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0124 - acc: 0.9949 - val_loss: 0.0117 - val_acc: 0.9961\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0192 - acc: 0.9946 - val_loss: 0.0076 - val_acc: 0.9961\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0100 - acc: 0.9978 - val_loss: 0.0118 - val_acc: 0.9961\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0210 - acc: 0.9956 - val_loss: 0.0058 - val_acc: 0.9981\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0114 - acc: 0.9959 - val_loss: 0.0105 - val_acc: 0.9961\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0116 - acc: 0.9956 - val_loss: 0.0079 - val_acc: 0.9981\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0124 - acc: 0.9965 - val_loss: 0.0083 - val_acc: 0.9961\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0094 - acc: 0.9972 - val_loss: 0.0073 - val_acc: 0.9981\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0109 - acc: 0.9965 - val_loss: 0.0087 - val_acc: 0.9961\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.0148 - acc: 0.9940 - val_loss: 0.0115 - val_acc: 0.9942\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0122 - acc: 0.9968 - val_loss: 0.0087 - val_acc: 0.9961\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0126 - acc: 0.9972 - val_loss: 0.0057 - val_acc: 0.9981\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0127 - acc: 0.9959 - val_loss: 0.0084 - val_acc: 0.9961\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0165 - acc: 0.9949 - val_loss: 0.0076 - val_acc: 0.9981\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0111 - acc: 0.9965 - val_loss: 0.0067 - val_acc: 0.9981\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0142 - acc: 0.9956 - val_loss: 0.0065 - val_acc: 0.9981\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0148 - acc: 0.9965 - val_loss: 0.0063 - val_acc: 0.9981\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0085 - acc: 0.9975 - val_loss: 0.0077 - val_acc: 0.9981\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0206 - acc: 0.9921 - val_loss: 0.0059 - val_acc: 0.9981\n",
      "Epoch 00063: early stopping\n",
      "Complete!\n",
      "Experiment #7\n",
      "Epoch 1/300\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 9s 94ms/step - loss: 0.6416 - acc: 0.6357 - val_loss: 0.5356 - val_acc: 0.7115\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.5385 - acc: 0.7352 - val_loss: 0.3097 - val_acc: 0.8538\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.4116 - acc: 0.8201 - val_loss: 0.3077 - val_acc: 0.8363\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.2642 - acc: 0.9069 - val_loss: 0.2218 - val_acc: 0.8967\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.1765 - acc: 0.9343 - val_loss: 0.1874 - val_acc: 0.9240\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.1457 - acc: 0.9511 - val_loss: 0.1600 - val_acc: 0.9337\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.1180 - acc: 0.9605 - val_loss: 0.1089 - val_acc: 0.9610\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.1096 - acc: 0.9634 - val_loss: 0.0358 - val_acc: 0.9883\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0845 - acc: 0.9688 - val_loss: 0.0743 - val_acc: 0.9688\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0525 - acc: 0.9842 - val_loss: 0.0406 - val_acc: 0.9825\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0461 - acc: 0.9858 - val_loss: 0.0623 - val_acc: 0.9766\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0542 - acc: 0.9842 - val_loss: 0.0602 - val_acc: 0.9708\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0422 - acc: 0.9858 - val_loss: 0.0781 - val_acc: 0.9688\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0286 - acc: 0.9893 - val_loss: 0.0554 - val_acc: 0.9825\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0376 - acc: 0.9883 - val_loss: 0.0436 - val_acc: 0.9903\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0373 - acc: 0.9890 - val_loss: 0.0433 - val_acc: 0.9864\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0367 - acc: 0.9877 - val_loss: 0.0334 - val_acc: 0.9903\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0213 - acc: 0.9937 - val_loss: 0.0463 - val_acc: 0.9903\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0276 - acc: 0.9905 - val_loss: 0.0269 - val_acc: 0.9922\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0220 - acc: 0.9949 - val_loss: 0.0296 - val_acc: 0.9922\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0213 - acc: 0.9924 - val_loss: 0.0260 - val_acc: 0.9942\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0164 - acc: 0.9943 - val_loss: 0.0269 - val_acc: 0.9942\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0163 - acc: 0.9959 - val_loss: 0.0306 - val_acc: 0.9942\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0251 - acc: 0.9927 - val_loss: 0.0217 - val_acc: 0.9961\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0201 - acc: 0.9924 - val_loss: 0.0247 - val_acc: 0.9942\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0120 - acc: 0.9978 - val_loss: 0.0260 - val_acc: 0.9942\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0179 - acc: 0.9956 - val_loss: 0.0211 - val_acc: 0.9942\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0229 - acc: 0.9934 - val_loss: 0.0239 - val_acc: 0.9961\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0246 - acc: 0.9927 - val_loss: 0.0260 - val_acc: 0.9942\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0146 - acc: 0.9943 - val_loss: 0.0287 - val_acc: 0.9942\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0169 - acc: 0.9946 - val_loss: 0.0244 - val_acc: 0.9942\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0129 - acc: 0.9962 - val_loss: 0.0265 - val_acc: 0.9942\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0130 - acc: 0.9965 - val_loss: 0.0286 - val_acc: 0.9942\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0204 - acc: 0.9946 - val_loss: 0.0215 - val_acc: 0.9942\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0202 - acc: 0.9943 - val_loss: 0.0242 - val_acc: 0.9942\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0080 - acc: 0.9975 - val_loss: 0.0197 - val_acc: 0.9942\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0208 - acc: 0.9931 - val_loss: 0.0167 - val_acc: 0.9942\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0140 - acc: 0.9962 - val_loss: 0.0231 - val_acc: 0.9942\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0147 - acc: 0.9959 - val_loss: 0.0201 - val_acc: 0.9942\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0151 - acc: 0.9965 - val_loss: 0.0232 - val_acc: 0.9942\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.0139 - acc: 0.9956 - val_loss: 0.0220 - val_acc: 0.9942\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0128 - acc: 0.9962 - val_loss: 0.0187 - val_acc: 0.9942\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0157 - acc: 0.9953 - val_loss: 0.0212 - val_acc: 0.9942\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0080 - acc: 0.9972 - val_loss: 0.0230 - val_acc: 0.9942\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0134 - acc: 0.9956 - val_loss: 0.0205 - val_acc: 0.9942\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0138 - acc: 0.9962 - val_loss: 0.0236 - val_acc: 0.9942\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0165 - acc: 0.9946 - val_loss: 0.0209 - val_acc: 0.9942\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0157 - acc: 0.9953 - val_loss: 0.0232 - val_acc: 0.9942\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 0.003125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0145 - acc: 0.9962 - val_loss: 0.0214 - val_acc: 0.9942\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0082 - acc: 0.9978 - val_loss: 0.0213 - val_acc: 0.9942\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0084 - acc: 0.9975 - val_loss: 0.0226 - val_acc: 0.9942\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0134 - acc: 0.9965 - val_loss: 0.0236 - val_acc: 0.9942\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0119 - acc: 0.9959 - val_loss: 0.0219 - val_acc: 0.9942\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0101 - acc: 0.9975 - val_loss: 0.0237 - val_acc: 0.9942\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0074 - acc: 0.9978 - val_loss: 0.0228 - val_acc: 0.9942\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0102 - acc: 0.9968 - val_loss: 0.0230 - val_acc: 0.9942\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0150 - acc: 0.9965 - val_loss: 0.0244 - val_acc: 0.9942\n",
      "Epoch 00057: early stopping\n",
      "Complete!\n",
      "Experiment #8\n",
      "Epoch 1/300\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 9s 88ms/step - loss: 0.6252 - acc: 0.6465 - val_loss: 0.5476 - val_acc: 0.6608\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.4822 - acc: 0.7664 - val_loss: 0.3199 - val_acc: 0.8480\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.3285 - acc: 0.8696 - val_loss: 0.2142 - val_acc: 0.8655\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.2322 - acc: 0.9100 - val_loss: 0.1246 - val_acc: 0.9396\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.1903 - acc: 0.9283 - val_loss: 0.1471 - val_acc: 0.9454\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.1307 - acc: 0.9549 - val_loss: 0.1227 - val_acc: 0.9513\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.1075 - acc: 0.9637 - val_loss: 0.3292 - val_acc: 0.8791\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0824 - acc: 0.9719 - val_loss: 0.0865 - val_acc: 0.9610\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.0753 - acc: 0.9725 - val_loss: 0.0900 - val_acc: 0.9708\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0475 - acc: 0.9864 - val_loss: 0.0578 - val_acc: 0.9786\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0538 - acc: 0.9833 - val_loss: 0.0393 - val_acc: 0.9903\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0430 - acc: 0.9867 - val_loss: 0.0501 - val_acc: 0.9805\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0359 - acc: 0.9871 - val_loss: 0.0425 - val_acc: 0.9883\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0350 - acc: 0.9896 - val_loss: 0.0270 - val_acc: 0.9883\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0455 - acc: 0.9883 - val_loss: 0.0348 - val_acc: 0.9883\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0419 - acc: 0.9880 - val_loss: 0.0468 - val_acc: 0.9844\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0360 - acc: 0.9896 - val_loss: 0.0320 - val_acc: 0.9903\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0316 - acc: 0.9905 - val_loss: 0.0264 - val_acc: 0.9903\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0274 - acc: 0.9921 - val_loss: 0.0299 - val_acc: 0.9903\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0276 - acc: 0.9918 - val_loss: 0.0405 - val_acc: 0.9864\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0256 - acc: 0.9931 - val_loss: 0.0224 - val_acc: 0.9942\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0140 - acc: 0.9959 - val_loss: 0.0369 - val_acc: 0.9883\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0303 - acc: 0.9918 - val_loss: 0.0508 - val_acc: 0.9844\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0222 - acc: 0.9924 - val_loss: 0.0248 - val_acc: 0.9942\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0177 - acc: 0.9949 - val_loss: 0.0329 - val_acc: 0.9903\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0254 - acc: 0.9905 - val_loss: 0.0235 - val_acc: 0.9942\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0309 - acc: 0.9915 - val_loss: 0.0318 - val_acc: 0.9903\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.0201 - acc: 0.994 - 2s 24ms/step - loss: 0.0222 - acc: 0.9943 - val_loss: 0.0254 - val_acc: 0.9922\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0261 - acc: 0.9927 - val_loss: 0.0256 - val_acc: 0.9942\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0303 - acc: 0.9946 - val_loss: 0.0258 - val_acc: 0.9903\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0140 - acc: 0.9949 - val_loss: 0.0263 - val_acc: 0.9922\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.00625.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0164 - acc: 0.9965 - val_loss: 0.0238 - val_acc: 0.9922\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0235 - acc: 0.9915 - val_loss: 0.0227 - val_acc: 0.9942\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0146 - acc: 0.9956 - val_loss: 0.0207 - val_acc: 0.9903\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0218 - acc: 0.9915 - val_loss: 0.0295 - val_acc: 0.9883\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0177 - acc: 0.9956 - val_loss: 0.0324 - val_acc: 0.9903\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0294 - acc: 0.9924 - val_loss: 0.0260 - val_acc: 0.9922\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0171 - acc: 0.9949 - val_loss: 0.0246 - val_acc: 0.9922\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.00625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0248 - acc: 0.9946 - val_loss: 0.0215 - val_acc: 0.9942\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0231 - acc: 0.9940 - val_loss: 0.0263 - val_acc: 0.9942\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0226 - acc: 0.9946 - val_loss: 0.0230 - val_acc: 0.9942\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0181 - acc: 0.9956 - val_loss: 0.0227 - val_acc: 0.9942\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0139 - acc: 0.9965 - val_loss: 0.0180 - val_acc: 0.9903\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0171 - acc: 0.9949 - val_loss: 0.0240 - val_acc: 0.9922\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0116 - acc: 0.9959 - val_loss: 0.0261 - val_acc: 0.9903\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0155 - acc: 0.9959 - val_loss: 0.0266 - val_acc: 0.9903\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0144 - acc: 0.9962 - val_loss: 0.0314 - val_acc: 0.9903\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0133 - acc: 0.9956 - val_loss: 0.0265 - val_acc: 0.9942\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 0.003125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0141 - acc: 0.9959 - val_loss: 0.0235 - val_acc: 0.9942\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0180 - acc: 0.9946 - val_loss: 0.0228 - val_acc: 0.9922\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0158 - acc: 0.9953 - val_loss: 0.0251 - val_acc: 0.9903\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0143 - acc: 0.9956 - val_loss: 0.0226 - val_acc: 0.9922\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0170 - acc: 0.9956 - val_loss: 0.0233 - val_acc: 0.9903\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0214 - acc: 0.9940 - val_loss: 0.0228 - val_acc: 0.9922\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0117 - acc: 0.9965 - val_loss: 0.0226 - val_acc: 0.9942\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0188 - acc: 0.9956 - val_loss: 0.0195 - val_acc: 0.9922\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - ETA: 0s - loss: 0.0280 - acc: 0.9926- ETA: 1s - loss:  - 2s 23ms/step - loss: 0.0278 - acc: 0.9927 - val_loss: 0.0145 - val_acc: 0.9922\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0147 - acc: 0.9965 - val_loss: 0.0235 - val_acc: 0.9942\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 0.0015625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0211 - acc: 0.9946 - val_loss: 0.0174 - val_acc: 0.9942\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0150 - acc: 0.9956 - val_loss: 0.0178 - val_acc: 0.9942\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0215 - acc: 0.9940 - val_loss: 0.0179 - val_acc: 0.9942\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0114 - acc: 0.9965 - val_loss: 0.0204 - val_acc: 0.9942\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0108 - acc: 0.9968 - val_loss: 0.0206 - val_acc: 0.9942\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0122 - acc: 0.9962 - val_loss: 0.0211 - val_acc: 0.9942\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0224 - acc: 0.9949 - val_loss: 0.0200 - val_acc: 0.9942\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0085 - acc: 0.9981 - val_loss: 0.0207 - val_acc: 0.9942\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0120 - acc: 0.9962 - val_loss: 0.0205 - val_acc: 0.9942\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0146 - acc: 0.9972 - val_loss: 0.0197 - val_acc: 0.9942\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to 0.00078125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0214 - acc: 0.9946 - val_loss: 0.0171 - val_acc: 0.9942\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to 0.000390625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0180 - acc: 0.9965 - val_loss: 0.0188 - val_acc: 0.9942\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to 0.000390625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0177 - acc: 0.9949 - val_loss: 0.0191 - val_acc: 0.9942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/300\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to 0.000390625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0166 - acc: 0.9953 - val_loss: 0.0192 - val_acc: 0.9942\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to 0.000390625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0167 - acc: 0.9959 - val_loss: 0.0203 - val_acc: 0.9942\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to 0.000390625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0133 - acc: 0.9968 - val_loss: 0.0197 - val_acc: 0.9942\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to 0.000390625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0183 - acc: 0.9946 - val_loss: 0.0187 - val_acc: 0.9942\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to 0.000390625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0176 - acc: 0.9946 - val_loss: 0.0195 - val_acc: 0.9942\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to 0.000390625.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0154 - acc: 0.9959 - val_loss: 0.0194 - val_acc: 0.9942\n",
      "Epoch 00077: early stopping\n",
      "Complete!\n",
      "Experiment #9\n",
      "Epoch 1/300\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 10s 97ms/step - loss: 0.6478 - acc: 0.6199 - val_loss: 0.5439 - val_acc: 0.7057\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 3s 27ms/step - loss: 0.5460 - acc: 0.7206 - val_loss: 0.6698 - val_acc: 0.6667\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 3s 27ms/step - loss: 0.5032 - acc: 0.7554 - val_loss: 0.6081 - val_acc: 0.6803\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.4040 - acc: 0.8226 - val_loss: 0.5153 - val_acc: 0.7349\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.2572 - acc: 0.9056 - val_loss: 0.2596 - val_acc: 0.8908\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.2000 - acc: 0.9268 - val_loss: 0.0487 - val_acc: 0.9825\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.1210 - acc: 0.9580 - val_loss: 0.2412 - val_acc: 0.9123\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.1144 - acc: 0.9631 - val_loss: 0.1982 - val_acc: 0.9201\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.05.\n",
      "99/99 [==============================] - 3s 25ms/step - loss: 0.0959 - acc: 0.9732 - val_loss: 0.1292 - val_acc: 0.9493\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.0729 - acc: 0.9751 - val_loss: 0.0595 - val_acc: 0.9747\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0581 - acc: 0.9807 - val_loss: 0.0814 - val_acc: 0.9688\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0556 - acc: 0.9823 - val_loss: 0.0961 - val_acc: 0.9649\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.0436 - acc: 0.9864 - val_loss: 0.1230 - val_acc: 0.9610\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0473 - acc: 0.9852 - val_loss: 0.0538 - val_acc: 0.9825\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0385 - acc: 0.9864 - val_loss: 0.0680 - val_acc: 0.9727\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0369 - acc: 0.9883 - val_loss: 0.0814 - val_acc: 0.9766\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0356 - acc: 0.9905 - val_loss: 0.0712 - val_acc: 0.9805\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0413 - acc: 0.9880 - val_loss: 0.0586 - val_acc: 0.9766\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.025.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0297 - acc: 0.9886 - val_loss: 0.0597 - val_acc: 0.9844\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 24ms/step - loss: 0.0251 - acc: 0.9915 - val_loss: 0.0657 - val_acc: 0.9805\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 3s 26ms/step - loss: 0.0214 - acc: 0.9912 - val_loss: 0.0593 - val_acc: 0.9805\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.0237 - acc: 0.9931 - val_loss: 0.0622 - val_acc: 0.9825\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0355 - acc: 0.9912 - val_loss: 0.0618 - val_acc: 0.9825\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0266 - acc: 0.9905 - val_loss: 0.0619 - val_acc: 0.9844\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 23ms/step - loss: 0.0231 - acc: 0.9934 - val_loss: 0.0582 - val_acc: 0.9825\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.0125.\n",
      "99/99 [==============================] - 2s 25ms/step - loss: 0.0254 - acc: 0.9937 - val_loss: 0.0526 - val_acc: 0.9825\n",
      "Epoch 00026: early stopping\n",
      "Complete!\n"
     ]
    }
   ],
   "source": [
    "for experiment in experiments:\n",
    "    print(\"Experiment #{}\".format(experiment))\n",
    "    model = build_model()\n",
    "    model.compile(loss='binary_crossentropy', optimizer=SGD(lr=0.05, momentum = 0.005, nesterov=True), metrics=['accuracy'])\n",
    "    \n",
    "    x_train, x_val, y_train, y_val = validation_set_generator(x_train2, y_train2, test_size=0.1)\n",
    "\n",
    "    history = model.fit_generator(\n",
    "        batch_generator(x_train, y_train, batch_size=b_size), \n",
    "        steps_per_epoch=x_train.shape[1]//b_size,\n",
    "        epochs=300,\n",
    "        validation_data=(x_val, y_val),\n",
    "        shuffle=True,\n",
    "        callbacks = [early_stopping, lr_schedule]\n",
    "        )\n",
    "         \n",
    "    historyname = \"vm_history_{}\".format(experiment)\n",
    "    with open('Models/vm_histories/{}'.format(historyname), 'wb') as file_pi:\n",
    "        pkl.dump(history.history, file_pi)    \n",
    "    \n",
    "    model.save(\"C:/Users/shark/Desktop/Big Data Analytics/Big Data Analytics project/Models/validated_model{}\".format(experiment))\n",
    "    print(\"Complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next steps also take some time to complete. They evaluate the models and extract relevant data for analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialise list-containers for models and their histories. \n",
    "\n",
    "models = []\n",
    "histories = []\n",
    "\n",
    "#Load the models and histories \n",
    "for x in experiments:\n",
    "    models.append(keras.models.load_model(\"C:/Users/shark/Desktop/Big Data Analytics/Big Data Analytics project/Models/validated_model{}\".format(x)))\n",
    "    histories.append(pkl.load(open('Models/vm_histories/vm_history_{}'.format(x), 'rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "570/570 [==============================] - 1s 2ms/step\n",
      "570/570 [==============================] - 1s 2ms/step\n",
      "570/570 [==============================] - 1s 2ms/step\n",
      "570/570 [==============================] - 1s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "#Initialise containers for result statistics and indices of misclassified stars for each of the experiments. \n",
    "results = np.zeros(12)\n",
    "fp_idxs = []\n",
    "fn_idxs = []\n",
    "\n",
    "for model in models:\n",
    "    #for the model from each experiment: .....\n",
    "    \n",
    "    #get the loss\n",
    "    loss = model.evaluate(x_test, y_test)[0]\n",
    "    \n",
    "    #get our predictions for our training set\n",
    "    y_hat = model.predict(x_test)[:,0]\n",
    "    guesses = np.round_(y_hat, 0)\n",
    "    \n",
    "    #Get the confusion matrix\n",
    "    cf = skl.confusion_matrix(y_test, guesses, labels=None, sample_weight=None)\n",
    "\n",
    "    #get other statistics\n",
    "    total_n = np.sum(cf)\n",
    "    fps = cf[0][1]\n",
    "    fns = cf[1][0]\n",
    "    tps = cf[1][1]\n",
    "    tns = cf[0][0]\n",
    "    \n",
    "    acc = (tps+tns)/(total_n)\n",
    "    tpr = tps/(tps+fns)\n",
    "    fpr = fps/(tns+fps)\n",
    "    spec = 1-fpr\n",
    "    prec = tps/(tps+fps)\n",
    "    B = 10\n",
    "    F10 = (1+B^2)*((prec*tpr)/(((B^2)*prec)+tpr))\n",
    "    \n",
    "    #append them all to our totals\n",
    "    results = np.vstack((results, np.array([total_n, fps, fns, tps, tns, acc, tpr, fpr, spec, prec, F10, loss])))\n",
    "    \n",
    "    #get indices of misclassed\n",
    "    first_test_np_idx = len(y_test[y_test == 1])\n",
    "    fp_idx = [x+first_test_np_idx for x in np.where(y_test[first_test_np_idx:,0] != guesses[first_test_np_idx:])]\n",
    "    fn_idx = [x for x in np.where(y_test[:first_test_np_idx:,0] != guesses[:first_test_np_idx])]\n",
    "    \n",
    "    #append them to our stores of misclassed examples\n",
    "    fp_idxs.append(fp_idx)\n",
    "    fn_idxs.append(fn_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TP</th>\n",
       "      <th>TN</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>TPR</th>\n",
       "      <th>FPR</th>\n",
       "      <th>Specificty</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F10-score</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>556.0</td>\n",
       "      <td>0.984211</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.015929</td>\n",
       "      <td>0.984071</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.013206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>555.0</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.017699</td>\n",
       "      <td>0.982301</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.012833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>557.0</td>\n",
       "      <td>0.985965</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.014159</td>\n",
       "      <td>0.985841</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.849057</td>\n",
       "      <td>0.013101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>556.0</td>\n",
       "      <td>0.984211</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.015929</td>\n",
       "      <td>0.984071</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.013240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average</th>\n",
       "      <td>570.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>556.0</td>\n",
       "      <td>0.984211</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.015929</td>\n",
       "      <td>0.984071</td>\n",
       "      <td>0.358059</td>\n",
       "      <td>0.833476</td>\n",
       "      <td>0.013095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Total    FP   FN   TP     TN  Accuracy  TPR       FPR  Specificty  \\\n",
       "0        570.0   9.0  0.0  5.0  556.0  0.984211  1.0  0.015929    0.984071   \n",
       "1        570.0  10.0  0.0  5.0  555.0  0.982456  1.0  0.017699    0.982301   \n",
       "2        570.0   8.0  0.0  5.0  557.0  0.985965  1.0  0.014159    0.985841   \n",
       "3        570.0   9.0  0.0  5.0  556.0  0.984211  1.0  0.015929    0.984071   \n",
       "Average  570.0   9.0  0.0  5.0  556.0  0.984211  1.0  0.015929    0.984071   \n",
       "\n",
       "         Precision  F10-score      loss  \n",
       "0         0.357143   0.833333  0.013206  \n",
       "1         0.333333   0.818182  0.012833  \n",
       "2         0.384615   0.849057  0.013101  \n",
       "3         0.357143   0.833333  0.013240  \n",
       "Average   0.358059   0.833476  0.013095  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [\"Total\", \"FP\", \"FN\", 'TP', 'TN', 'Accuracy', 'TPR', 'FPR', 'Specificty', 'Precision', 'F10-score', 'loss']\n",
    "df = pd.DataFrame(results)\n",
    "df = df.iloc[1:,:]\n",
    "df.columns = labels\n",
    "\n",
    "#Add average row and rename row\n",
    "df = df.append(df.mean(numeric_only=True), ignore_index=True)\n",
    "df = df.rename({len(experiments): 'Average'}, axis='index')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models are all almost perfect classifiers.\n",
    "\n",
    "We do not use accuracy as a measure of model quality since a model that predicts ‘no planet’ for every star will have an accuracy of 99.128% (565/570) on the test set.\n",
    "\n",
    "To evaluate model quality, we should decide which misclassification outcome (a false negative (FN) or false positive (FP)) is worse for our purposes, so we should consider what those purposes are:\n",
    "\n",
    "If our model is to be practically useful, it will be because it can evaluate more data more quickly at a lower cost than can be done using humans. When applying the model to new data, we might have trouble disagreeing with the CNN since it may have superhuman performance in identifying positive examples in large datasets, being able to notice subtle statistical patterns that a human might miss, yet it cannot explain its reasoning (At least, not by default. But interpretable ML methods have helped to this end). This is not a major issue, since closer analysis of the star by humans could negate any FPs returned, provided there are not too many of them. Obversely, since it is likely that we would use our model to sift through more data than humans can search by hand, with FNs we might miss an important scientific discovery! Let’s therefore decide that a small number of FPs is acceptable and FNs are not acceptable. This corresponds to a preference for a moderate precision and a perfect True Positive Rate (TPR). We would probably be willing to trade roughly 10 FPs to avoid 1 FN, but this will vary depending on one's purposes and resources.\n",
    "\n",
    "My models achieve a moderate average precision, 0.5923. Over 10 experiments, the average number of FPs per experiment was 3.7. Given the low prevalence of positive examples in the test data set and given our stated preferences, this is a strong result. The TPR is 0.98, corresponding to only one FN over 10 experiments, most trained models performing perfectly. This is a very strong result. To give a single statistic summary of the model, instead of using F1-score, which is used for more balanced data sets, we instead use Fβ-score, where β=10, making the F10-score, which ‘measures the effectiveness of retrieval with respect to a user who attaches β=10 times as much importance to TPR as precision’ [10]. F10-score is 0.910. The best being 1.0, this is a good score and we could consider our model successful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the histories on the same graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = plt.figure()\n",
    "for i in range(len(histories)):\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(histories[i]['loss'], color='b',label='Training set loss')\n",
    "    plt.plot(histories[i]['val_loss'], color='r',label='Test set loss')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.title(\"Loss\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(histories[i]['acc'], color='b',label='Training set accuracy')\n",
    "    plt.plot(histories[i]['val_acc'], color='r',label='Test set accuracy')\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.subplots_adjust(top=0.92, bottom=0, left=-2, right=0.95, hspace=0, wspace=0.35)\n",
    "    results.savefig('results_vm_2_lowervsplit.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For me, the models converged at approximately 20-40 epochs. The figure above shows the accuracy and loss convergence plots for the training (blue) and validation (red) sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting misclassifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count up the number of times each star was misclassfied\n",
    "fp_counter = np.zeros_like(y_test)\n",
    "fn_counter = np.zeros_like(y_test)\n",
    "\n",
    "for i in range(len(fp_idxs)):\n",
    "    for j in range(len(fp_idxs[i][0])):\n",
    "        fp_counter[fp_idxs[i][0][j]] += 1 \n",
    "fp_counter = fp_counter.transpose()[0]\n",
    "\n",
    "for i in range(len(fn_idxs)):\n",
    "    for j in range(len(fn_idxs[i][0])):\n",
    "        fn_counter[fn_idxs[i][0][j]] += 1 \n",
    "fn_counter = fn_counter.transpose()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fp_counter)\n",
    "print(fn_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = plt.figure()\n",
    "plt.plot(x_test[fp_counter.argmax()])\n",
    "plt.title(\"Most common False Positive\")\n",
    "results.savefig('star_124_fp.png', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
